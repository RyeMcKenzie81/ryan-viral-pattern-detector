# V1.2 Feature 3.1: Async Batch Generation Design

**Goal**: 5x speed improvement using async/await and concurrent batch processing

---

## Current Architecture (V1/V1.1)

**Synchronous Processing**:
```python
for tweet in tweets:
    result = generator.generate_suggestions(tweet)  # Blocks for 3-5 seconds
    save_to_db(result)
```

**Problems**:
- Sequential: waits for each API call to complete
- 100 tweets × 4 seconds = 400 seconds (6.7 minutes)
- Wastes time during network I/O

---

## New Architecture (V1.2)

**Asynchronous Batch Processing**:
```python
# Process 5-10 tweets concurrently
batch_size = 5
for batch in batches(tweets, size=batch_size):
    results = await asyncio.gather(*[
        generator.generate_suggestions_async(tweet)
        for tweet in batch
    ])
    save_results(results)
```

**Expected Performance**:
- 100 tweets ÷ 5 concurrent × 4 seconds = 80 seconds (1.3 minutes)
- **5x faster** than sequential

---

## Design Decisions

### 1. Async Strategy

**Option A: ThreadPoolExecutor (Simple)**
- Use `concurrent.futures.ThreadPoolExecutor`
- Minimal code changes
- Good for I/O-bound tasks (API calls)
- ✅ **SELECTED** - Best balance of simplicity and performance

**Option B: Full async/await (Complex)**
- Convert everything to async/await
- Requires async HTTP client (aiohttp)
- google-generativeai library is sync-only
- ❌ Rejected - too much refactoring

### 2. Rate Limiting

**Challenge**: Need to enforce 15 req/min across concurrent requests

**Solution**: Use `asyncio.Semaphore` + time tracking
```python
# Semaphore limits concurrent requests
semaphore = asyncio.Semaphore(5)  # Max 5 concurrent

# Track call times for rate limiting
async def rate_limited_call():
    async with semaphore:
        await wait_for_rate_limit()  # Check 15/min limit
        result = await run_in_executor(sync_api_call)
        record_call_time()
        return result
```

### 3. Backward Compatibility

**Keep Synchronous API**:
- Maintain existing `generate_suggestions()` method
- Add new `generate_suggestions_async()` method
- Users can choose sync or async

**Gradual Migration**:
- V1.2: Add async batch processing
- V1.3: Deprecate sync (if needed)

---

## Implementation Plan

### Phase 1: Create Async Wrapper
1. Add `AsyncCommentGenerator` class
2. Use `asyncio.to_thread()` to run sync API calls
3. Implement async-compatible rate limiting

### Phase 2: Batch Processing
1. Create `generate_batch_async()` method
2. Use `asyncio.Semaphore` for concurrency control
3. Process tweets in batches of 5

### Phase 3: CLI Integration
1. Add `--batch-size` parameter (default: 5)
2. Use `asyncio.run()` in CLI command
3. Add progress updates during batch processing

### Phase 4: Testing
1. Test with 50 tweets (10 batches)
2. Measure speed improvement
3. Verify rate limiting still works

---

## Code Structure

### New Files
- `viraltracker/generation/async_comment_generator.py` (NEW)
  - AsyncCommentGenerator class
  - Async rate limiting logic

### Modified Files
- `viraltracker/cli/twitter.py`
  - Update generate-comments command to use async
  - Add --batch-size parameter
  - Use asyncio.run()

---

## API Design

### AsyncCommentGenerator

```python
class AsyncCommentGenerator:
    def __init__(self, api_key=None, max_requests_per_minute=15, batch_size=5):
        self.sync_generator = CommentGenerator(api_key, max_requests_per_minute)
        self.batch_size = batch_size
        self.semaphore = asyncio.Semaphore(batch_size)

    async def generate_suggestions_async(self, tweet, topic, config):
        """Async wrapper around sync generate_suggestions"""
        async with self.semaphore:
            # Wait for rate limit
            await self._wait_for_rate_limit()

            # Run sync call in thread pool
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                None,  # Default ThreadPoolExecutor
                self.sync_generator.generate_suggestions,
                tweet, topic, config
            )

            # Record call for rate tracking
            self._record_call()
            return result

    async def generate_batch_async(self, tweets, topics, config):
        """Generate suggestions for multiple tweets concurrently"""
        tasks = [
            self.generate_suggestions_async(tweet, topic, config)
            for tweet, topic in zip(tweets, topics)
        ]
        return await asyncio.gather(*tasks, return_exceptions=True)
```

### CLI Usage

```bash
# Use default batch size (5 concurrent)
./vt twitter generate-comments --project my-project

# Custom batch size
./vt twitter generate-comments --project my-project --batch-size 10

# Disable batching (sequential)
./vt twitter generate-comments --project my-project --batch-size 1
```

---

## Performance Estimates

### Sequential (V1.1)
- 100 tweets × 4 seconds = 400 seconds
- Rate limit: 15 req/min = 1 req per 4 seconds
- Total time: **6.7 minutes**

### Batch Size 5 (V1.2)
- 100 tweets ÷ 5 = 20 batches
- 20 batches × 4 seconds = 80 seconds
- Total time: **1.3 minutes**
- **Speedup: 5x**

### Batch Size 10 (V1.2 Aggressive)
- 100 tweets ÷ 10 = 10 batches
- But rate limit: 15 req/min
- Every minute: process 15 tweets (1.5 batches)
- 100 tweets ÷ 15 per minute = 6.7 minutes
- Wait... this doesn't help beyond batch_size=7.5
- **Optimal batch size: 5-7**

---

## Rate Limiting Strategy

**Sliding Window with Async**:
```python
class AsyncRateLimiter:
    def __init__(self, max_per_minute=15):
        self.max_rpm = max_per_minute
        self.call_times = []
        self.lock = asyncio.Lock()

    async def wait_if_needed(self):
        async with self.lock:
            now = time.time()

            # Remove calls older than 1 minute
            self.call_times = [t for t in self.call_times if now - t < 60]

            # If at limit, wait
            if len(self.call_times) >= self.max_rpm:
                sleep_time = 60 - (now - self.call_times[0]) + 0.1
                if sleep_time > 0:
                    await asyncio.sleep(sleep_time)
                    # Clean up after waiting
                    now = time.time()
                    self.call_times = [t for t in self.call_times if now - t < 60]

    async def record_call(self):
        async with self.lock:
            self.call_times.append(time.time())
```

---

## Error Handling

**Handle Failures in Batch**:
```python
results = await asyncio.gather(*tasks, return_exceptions=True)

for tweet, result in zip(tweets, results):
    if isinstance(result, Exception):
        logger.error(f"Failed to process tweet {tweet.tweet_id}: {result}")
        # Continue with other tweets
    else:
        # Process successful result
        save_to_db(result)
```

---

## Testing Plan

### Unit Tests
1. Test AsyncCommentGenerator initialization
2. Test async rate limiting
3. Test batch processing with mock API

### Integration Tests
1. Test with 10 tweets (2 batches of 5)
2. Verify all suggestions saved to DB
3. Verify rate limit enforcement (15/min)

### Performance Tests
1. Process 50 tweets sequentially (V1.1 baseline)
2. Process 50 tweets with batch_size=5 (V1.2)
3. Measure and compare execution time
4. Target: 4-5x speedup

---

## Risks & Mitigations

### Risk 1: Rate Limit Violations
- **Mitigation**: Conservative semaphore + sliding window tracking
- **Testing**: Monitor API response codes for 429 errors

### Risk 2: Increased Error Rate
- **Mitigation**: Exponential backoff still works (per-request)
- **Testing**: Inject failures to verify resilience

### Risk 3: Database Contention
- **Mitigation**: Use upsert with conflict resolution
- **Testing**: Verify no duplicate records created

---

## Success Metrics

1. **Speed**: 4-5x faster than sequential
2. **Reliability**: Same success rate as sequential
3. **Rate Limit**: No 429 errors (respect 15/min)
4. **Quality**: Same suggestion quality

---

**Status**: Design Complete - Ready for Implementation
**Estimated Time**: 2-3 hours
**Priority**: HIGH (biggest performance gain in V1.2)
