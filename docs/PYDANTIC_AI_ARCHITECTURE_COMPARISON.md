# PydanticAI Architecture Comparison & Recommendations

**Date:** 2025-01-24
**Purpose:** Compare existing migration plans with PydanticAI best practices from official docs
**Documents Analyzed:**
- `PYDANTIC_AI_MIGRATION_PLAN.md` (Nov 20, 2025)
- `ORCHESTRATOR_REFACTOR_PLAN.md` (Nov 21, 2025)
- ChatGPT/PydanticAI suggested 5-layer structure

---

## Executive Summary

**Good News:** Your architecture is **85% aligned** with PydanticAI best practices! ‚úÖ

**What you have right:**
- ‚úÖ Services layer (Level 1: Dependencies)
- ‚úÖ Agent layer (Level 2: Agents as roles)
- ‚úÖ Tools layer (Level 3: Tools & toolsets)
- ‚úÖ Orchestration pattern (Level 4: Multi-agent orchestration)
- ‚úÖ UI/API integration (Level 5: Integrations)

**What needs enhancement:**
- ‚ö†Ô∏è **Level 0 (Settings & Models)** - Needs centralized model/provider config
- ‚ö†Ô∏è **Workflows (pydantic-graph)** - Missing graph-based orchestration
- ‚ö†Ô∏è **Observability** - Missing Logfire + pydantic_evals

**Bottom Line:** Your migration plan is solid. We recommend **minor additions** to reach 100% alignment.

---

## The 5-Layer PydanticAI Mental Model

Based on official docs, here's the recommended structure:

### Layer 0: Settings & Models
**Purpose:** Centralized configuration for LLM providers, models, and environment settings

**What PydanticAI recommends:**
```python
# viraltracker/config/settings.py
# viraltracker/config/models.py

# Centralized configuration for:
- Which LLMs to use (OpenAI, Anthropic, Gemini, etc.)
- Model aliases (e.g., "gateway/openai:gpt-5" vs direct "openai:gpt-5")
- Timeouts, retries, usage limits
- Gateway vs direct provider
```

**What you currently have:**
- ‚úÖ `viraltracker/core/config.py` with environment variable management
- ‚úÖ API keys centralized (GEMINI_API_KEY, OPENAI_API_KEY, etc.)
- ‚ö†Ô∏è **Missing:** Model configuration layer (model selection, aliases, usage limits)
- ‚ö†Ô∏è **Missing:** Provider profiles (dev/staging/prod model configs)

**Recommendation:** ‚ú® **Minor enhancement needed**

---

### Layer 1: Dependencies & Output Schemas
**Purpose:** One deps_type dataclass per domain + one output_type Pydantic model per agent

**What PydanticAI recommends:**
```python
# Per-domain dependencies
@dataclass
class SupportDependencies:
    db: Database
    config: Config

# Per-domain outputs
class SupportOutput(BaseModel):
    ticket_id: str
    resolution: str
```

**What you currently have:**
- ‚úÖ `AgentDependencies` dataclass in `viraltracker/agent/dependencies.py`
- ‚úÖ All services injected (TwitterService, GeminiService, TikTokService, etc.)
- ‚úÖ Pydantic models in `viraltracker/services/models.py`:
  - `Tweet`, `HookAnalysis`, `OutlierTweet`, `OutlierResult`, `HookAnalysisResult`
- ‚úÖ Per-domain output models already exist
- ‚ö†Ô∏è **Missing:** ResultCache for inter-agent communication (mentioned in ORCHESTRATOR_REFACTOR_PLAN.md but not yet implemented)

**Recommendation:** ‚ú® **Add ResultCache as planned in ORCHESTRATOR_REFACTOR_PLAN.md**

---

### Layer 2: Agents as Globally-Defined "Roles"
**Purpose:** One Agent per major "role" (specialist agents for domains)

**What PydanticAI recommends:**
```python
# Define one agent per major role
support_agent = Agent(...)
research_agent = Agent(...)
creative_agent = Agent(...)
routing_agent = Agent(...)
```

**What you currently have:**
- ‚úÖ Main agent in `viraltracker/agent/agent.py`
- ‚úÖ Uses PydanticAI `Agent()` class correctly
- ‚úÖ Proper `deps_type=AgentDependencies`
- ‚úÖ Static + dynamic instructions (system prompts)
- üìã **PLANNED:** Orchestrator refactor with 5 specialized agents:
  - `twitter_agent` (5 tools)
  - `tiktok_agent` (5 tools)
  - `youtube_agent` (1 tool)
  - `facebook_agent` (2 tools)
  - `analysis_agent` (3 tools)
  - `orchestrator` (coordinates all agents)

**Current State:** ‚úÖ **PARTIAL** - Single monolithic agent working
**Planned State:** ‚úÖ **EXCELLENT** - Multi-agent orchestrator pattern follows best practices

**Recommendation:** ‚ú® **Proceed with ORCHESTRATOR_REFACTOR_PLAN.md as designed**

---

### Layer 3: Tools & Toolsets
**Purpose:** Organize tools by capability, not by agent. Use toolsets for reusable bundles.

**What PydanticAI recommends:**
```python
# viraltracker/tools/
db_tools.py
search_tools.py
file_tools.py
mcp_tools.py

# Toolsets for reusable bundles
from pydantic_ai import ToolSet

twitter_toolset = ToolSet([
    search_twitter_tool,
    export_tweets_tool,
    analyze_tweet_tool
])
```

**What you currently have:**
- ‚úÖ `viraltracker/agent/tools.py` - Core analysis tools
- ‚úÖ `viraltracker/agent/tools_phase15.py` - Twitter/comment tools
- ‚úÖ `viraltracker/agent/tools_phase16.py` - TikTok tools
- ‚úÖ `viraltracker/agent/tools_phase17.py` - YouTube/Facebook tools
- ‚úÖ `viraltracker/agent/tools_registered.py` - Tool registration
- ‚úÖ All tools use `@agent.tool` decorator
- ‚ö†Ô∏è **Missing:** ToolSet pattern (optional but recommended for reusability)

**Recommendation:** ‚ú® **Optional enhancement: Create ToolSets**

---

### Layer 4: Orchestration (Multi-Agent + Workflows)
**Purpose:** Handle single-agent, agent delegation, programmatic hand-off, and graph-based control flow

**What PydanticAI recommends:**

4 levels of complexity:

1. **Single agent workflows** - Simple, one-shot agent calls
2. **Agent delegation** - Agent A calls Agent B via tool
3. **Programmatic hand-off** - Python orchestrator calls agents in sequence
4. **Graph-based control flow** - Use `pydantic-graph` for complex workflows

```python
# Example: Graph-based workflow
from pydantic_graph import GraphBuilder

g = GraphBuilder()

@g.step
async def fetch_tweets(ctx):
    # Call twitter_agent
    return await twitter_agent.run(...)

@g.step
async def analyze_hooks(ctx, tweets):
    # Call analysis_agent
    return await analysis_agent.run(...)

# Decisions, joins, parallel execution, reducers
```

**What you currently have:**

**Current:**
- ‚úÖ Single agent with 15+ tools (Level 1: ‚úÖ)
- ‚ö†Ô∏è No agent delegation (Level 2: ‚ùå)
- ‚ö†Ô∏è No programmatic hand-off (Level 3: ‚ùå)
- ‚ö†Ô∏è No graph-based workflows (Level 4: ‚ùå)

**Planned (ORCHESTRATOR_REFACTOR_PLAN.md):**
- ‚úÖ Agent delegation via orchestrator (Level 2: ‚úÖ)
- ‚úÖ Programmatic hand-off via Python orchestrator (Level 3: ‚úÖ)
- ‚ö†Ô∏è No graph-based workflows (Level 4: ‚ùå)

**Recommendation:**
- ‚ú® **Phase 1:** Implement orchestrator pattern as planned (Levels 1-3)
- ‚ú® **Phase 2 (Optional):** Add `pydantic-graph` for complex multi-step workflows (Level 4)

---

### Layer 5: Integrations (UI, Durable Execution, Evals, Observability)
**Purpose:** External integrations for UI, persistence, evaluation, and monitoring

**What PydanticAI recommends:**

**UI:**
- FastAPI chat endpoints
- Streamlit UI
- AG-UI / Vercel AI integration

**Durable Execution:**
- Temporal / Prefect / DBOS integration
- Built-in `durable_exec` for simple cases

**Evaluations:**
- `pydantic_evals` for eval suites
- Dataset definitions + evaluators

**Observability:**
- `Logfire` for tracing and metrics
- `logfire.instrument_pydantic_ai()` for automatic instrumentation

**What you currently have:**

**UI:**
- ‚úÖ `viraltracker/ui/app.py` - Streamlit chat interface (COMPLETE)
- ‚úÖ `viraltracker/api/app.py` - FastAPI endpoints (COMPLETE)
- ‚úÖ Multi-page Streamlit (Tools Catalog, Database Browser, History, Services Catalog)

**Durable Execution:**
- ‚ö†Ô∏è **Missing:** No Temporal/Prefect/DBOS integration
- ‚ö†Ô∏è **Missing:** No `durable_exec` usage

**Evaluations:**
- ‚ö†Ô∏è **Missing:** No `pydantic_evals` integration
- ‚ö†Ô∏è **Missing:** No eval datasets or test suites

**Observability:**
- ‚ö†Ô∏è **Missing:** No Logfire integration
- ‚ö†Ô∏è **Missing:** No instrumentation for agent/tool calls
- ‚úÖ Basic logging via Python `logging` module

**Recommendation:**
- ‚ú® **High Priority:** Add Logfire instrumentation (simple setup, huge benefits)
- ‚ú® **Medium Priority:** Add pydantic_evals for regression testing
- ‚ú® **Low Priority:** Durable execution (only needed if workflows cross processes/time)

---

## Gap Analysis: What's Missing?

### Critical Gaps (Should Add)
None! Your core architecture is solid.

### Important Gaps (Recommended to Add)

1. **Level 0: Model Configuration Layer**
   - **Status:** ‚ö†Ô∏è Missing
   - **Impact:** Medium - Makes model switching and testing harder
   - **Effort:** Low (2-3 hours)
   - **File:** `viraltracker/config/models.py`

2. **Level 1: ResultCache for Inter-Agent Communication**
   - **Status:** ‚ö†Ô∏è Planned but not implemented
   - **Impact:** High - Needed for orchestrator pattern
   - **Effort:** Low (1-2 hours)
   - **File:** `viraltracker/agent/dependencies.py`

3. **Level 5: Logfire Instrumentation**
   - **Status:** ‚ö†Ô∏è Missing
   - **Impact:** High - Essential for debugging and monitoring agents
   - **Effort:** Low (1-2 hours setup, automatic after)
   - **File:** `viraltracker/core/instrumentation.py`

4. **Level 5: Pydantic Evals**
   - **Status:** ‚ö†Ô∏è Missing
   - **Impact:** Medium - Important for regression testing
   - **Effort:** Medium (4-6 hours for initial setup)
   - **File:** `viraltracker/evals/`

### Optional Gaps (Nice to Have)

5. **Level 3: ToolSets Pattern**
   - **Status:** ‚ö†Ô∏è Not using ToolSet class
   - **Impact:** Low - Current approach works fine
   - **Effort:** Low (2-3 hours refactor)
   - **File:** `viraltracker/agent/toolsets/`

6. **Level 4: Pydantic Graph**
   - **Status:** ‚ö†Ô∏è Missing
   - **Impact:** Low - Only needed for complex multi-step workflows
   - **Effort:** High (8-12 hours to learn + implement)
   - **File:** `viraltracker/workflows/graphs/`

7. **Level 5: Durable Execution**
   - **Status:** ‚ö†Ô∏è Missing
   - **Impact:** Low - Only needed if workflows are long-running or cross processes
   - **Effort:** High (12-20 hours for Temporal/Prefect setup)
   - **File:** `viraltracker/orchestration/durable.py`

---

## Recommended Project Structure (Updated)

Based on PydanticAI best practices + your current architecture:

```
viraltracker/
‚îú‚îÄ‚îÄ config/                          # NEW: Level 0 - Settings & Models
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ settings.py                  # Environment, keys, gateway URLs
‚îÇ   ‚îú‚îÄ‚îÄ models.py                    # Model aliases, profiles, usage limits
‚îÇ   ‚îî‚îÄ‚îÄ providers.py                 # Provider configuration (OpenAI, Anthropic, Gemini)
‚îÇ
‚îú‚îÄ‚îÄ services/                        # ‚úÖ EXISTING: Level 1 - Business logic
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ twitter_service.py
‚îÇ   ‚îú‚îÄ‚îÄ gemini_service.py
‚îÇ   ‚îú‚îÄ‚îÄ stats_service.py
‚îÇ   ‚îú‚îÄ‚îÄ tiktok_service.py
‚îÇ   ‚îú‚îÄ‚îÄ youtube_service.py
‚îÇ   ‚îú‚îÄ‚îÄ facebook_service.py
‚îÇ   ‚îî‚îÄ‚îÄ models.py                    # Pydantic output models
‚îÇ
‚îú‚îÄ‚îÄ agent/                           # ‚úÖ EXISTING: Level 2-3 - Agents & Tools
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ dependencies.py              # UPDATE: Add ResultCache
‚îÇ   ‚îú‚îÄ‚îÄ agent.py                     # Current: Monolithic agent
‚îÇ   ‚îÇ                                # UPDATE: Will export orchestrator
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ agents/                      # NEW: Specialized agents
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ twitter_agent.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tiktok_agent.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ youtube_agent.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ facebook_agent.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ analysis_agent.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ orchestrator.py              # NEW: Level 4 - Orchestrator pattern
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ tools/                       # ‚úÖ EXISTING (refactor organization)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ twitter_tools.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tiktok_tools.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ youtube_tools.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ facebook_tools.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ analysis_tools.py
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ toolsets/                    # NEW: Optional ToolSets
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ twitter_toolset.py
‚îÇ       ‚îî‚îÄ‚îÄ tiktok_toolset.py
‚îÇ
‚îú‚îÄ‚îÄ workflows/                       # NEW: Level 4 - Graph-based workflows (optional)
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ graphs.py
‚îÇ   ‚îú‚îÄ‚îÄ onboarding_flow.py
‚îÇ   ‚îî‚îÄ‚îÄ analysis_pipeline.py
‚îÇ
‚îú‚îÄ‚îÄ api/                             # ‚úÖ EXISTING: Level 5 - FastAPI
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ app.py
‚îÇ   ‚îî‚îÄ‚îÄ routes/
‚îÇ
‚îú‚îÄ‚îÄ ui/                              # ‚úÖ EXISTING: Level 5 - Streamlit
‚îÇ   ‚îú‚îÄ‚îÄ app.py
‚îÇ   ‚îî‚îÄ‚îÄ pages/
‚îÇ
‚îú‚îÄ‚îÄ evals/                           # NEW: Level 5 - Evaluations
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ datasets/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ twitter_evals.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tiktok_evals.py
‚îÇ   ‚îú‚îÄ‚îÄ suites.py
‚îÇ   ‚îî‚îÄ‚îÄ reports.py
‚îÇ
‚îú‚îÄ‚îÄ core/                            # ‚úÖ EXISTING: Core utilities
‚îÇ   ‚îú‚îÄ‚îÄ config.py                    # Keep existing, but move model config to config/
‚îÇ   ‚îî‚îÄ‚îÄ instrumentation.py           # NEW: Logfire setup
‚îÇ
‚îú‚îÄ‚îÄ cli/                             # ‚úÖ EXISTING: CLI
‚îÇ   ‚îî‚îÄ‚îÄ main.py
‚îÇ
‚îî‚îÄ‚îÄ tests/                           # ‚úÖ EXISTING: Tests
    ‚îú‚îÄ‚îÄ agent/
    ‚îú‚îÄ‚îÄ services/
    ‚îî‚îÄ‚îÄ integration/
```

---

## Comparison: Your Plans vs PydanticAI Best Practices

### PYDANTIC_AI_MIGRATION_PLAN.md (Nov 20, 2025)

**Alignment Score: 90% ‚úÖ**

**What aligns:**
- ‚úÖ Services layer (Level 1)
- ‚úÖ Agent with deps_type (Level 2)
- ‚úÖ Tools with @agent.tool (Level 3)
- ‚úÖ Streamlit + FastAPI (Level 5)
- ‚úÖ Result validators
- ‚úÖ Structured outputs (Pydantic models)

**What's missing:**
- ‚ö†Ô∏è Model configuration layer (Level 0)
- ‚ö†Ô∏è Multi-agent orchestration (Level 4)
- ‚ö†Ô∏è Logfire instrumentation (Level 5)
- ‚ö†Ô∏è Pydantic evals (Level 5)

**Verdict:** ‚úÖ **Excellent foundation**, ready for orchestrator refactor

---

### ORCHESTRATOR_REFACTOR_PLAN.md (Nov 21, 2025)

**Alignment Score: 95% ‚úÖ**

**What aligns:**
- ‚úÖ ResultCache for inter-agent communication (Level 1)
- ‚úÖ 5 specialized agents (Level 2)
- ‚úÖ Agent delegation via orchestrator (Level 4)
- ‚úÖ Programmatic hand-off (Level 4)
- ‚úÖ Backwards compatibility (agent = orchestrator export)

**What's missing:**
- ‚ö†Ô∏è Model configuration layer (Level 0)
- ‚ö†Ô∏è Graph-based workflows with pydantic-graph (Level 4, optional)
- ‚ö†Ô∏è Logfire instrumentation (Level 5)
- ‚ö†Ô∏è Pydantic evals (Level 5)

**Verdict:** ‚úÖ **Excellent design**, follows multi-agent best practices perfectly

---

## Action Plan: Reaching 100% Alignment

### Phase 1: Complete Orchestrator Refactor (Already Planned ‚úÖ)

**Goal:** Implement multi-agent orchestration pattern

**Files:**
1. ‚úÖ Update `viraltracker/agent/dependencies.py` - Add ResultCache
2. ‚úÖ Create `viraltracker/agent/agents/` directory
3. ‚úÖ Create 5 specialized agents (twitter, tiktok, youtube, facebook, analysis)
4. ‚úÖ Create `viraltracker/agent/orchestrator.py`
5. ‚úÖ Update `viraltracker/agent/agent.py` to export orchestrator

**Timeline:** 2-3 days (as planned)

**Status:** ‚úÖ **Ready to implement** - ORCHESTRATOR_REFACTOR_PLAN.md is excellent

---

### Phase 2: Add Missing Layer 0 (Model Configuration)

**Goal:** Centralize model/provider configuration for easier switching

**New Files:**
```
viraltracker/config/
‚îú‚îÄ‚îÄ settings.py      # Env vars, gateway URLs
‚îú‚îÄ‚îÄ models.py        # Model aliases, usage limits
‚îî‚îÄ‚îÄ providers.py     # Provider configs
```

**Example: `viraltracker/config/models.py`**
```python
"""Model configuration and profiles"""
from pydantic import BaseModel
from typing import Dict, Optional

class ModelConfig(BaseModel):
    """Configuration for a specific model"""
    provider: str  # 'openai', 'anthropic', 'gemini'
    model_name: str
    temperature: float = 0.7
    max_tokens: Optional[int] = None
    timeout: int = 60
    retries: int = 2

# Model aliases for easy switching
MODELS = {
    'fast': ModelConfig(
        provider='openai',
        model_name='gpt-4o-mini',
        temperature=0.5
    ),
    'smart': ModelConfig(
        provider='openai',
        model_name='gpt-5.1-2025-11-13',
        temperature=0.7
    ),
    'creative': ModelConfig(
        provider='anthropic',
        model_name='claude-sonnet-4',
        temperature=0.9
    ),
    'gemini': ModelConfig(
        provider='google',
        model_name='gemini-2.0-flash-exp',
        temperature=0.7
    )
}

# Environment-specific profiles
PROFILES = {
    'dev': 'fast',      # Use fast model in development
    'staging': 'smart', # Use smart model in staging
    'prod': 'smart'     # Use smart model in production
}

def get_model_config(profile: str = 'dev') -> ModelConfig:
    """Get model configuration for environment"""
    alias = PROFILES.get(profile, 'fast')
    return MODELS[alias]
```

**Usage in agents:**
```python
from viraltracker.config.models import get_model_config

model_config = get_model_config('prod')

agent = Agent(
    f'{model_config.provider}:{model_config.model_name}',
    deps_type=AgentDependencies,
    retries=model_config.retries
)
```

**Timeline:** 2-3 hours

**Benefits:**
- Easy model switching for testing
- Environment-specific configs (dev/staging/prod)
- Centralized usage limits and timeouts
- Easier A/B testing of models

---

### Phase 3: Add Logfire Instrumentation

**Goal:** Automatic tracing and monitoring of all agent/tool calls

**New File:** `viraltracker/core/instrumentation.py`

```python
"""Logfire instrumentation for agent observability"""
import os
import logfire
from typing import Optional

def setup_logfire(
    service_name: str = 'viraltracker',
    environment: str = 'development',
    enable_console: bool = True
) -> None:
    """
    Setup Logfire for agent tracing and monitoring.

    Args:
        service_name: Service identifier for Logfire
        environment: dev/staging/prod
        enable_console: Whether to print to console
    """
    # Configure Logfire
    logfire.configure(
        service_name=service_name,
        environment=environment,
        send_to_logfire=True,  # Send to Logfire cloud
        console=enable_console
    )

    # Instrument PydanticAI automatically
    logfire.instrument_pydantic_ai()

    print(f"‚úÖ Logfire configured for {service_name} ({environment})")


# Call this at app startup
# In viraltracker/ui/app.py:
# from viraltracker.core.instrumentation import setup_logfire
# setup_logfire(environment='production')
```

**Update in app startup:**
```python
# viraltracker/ui/app.py
from viraltracker.core.instrumentation import setup_logfire

# At the top of the file
setup_logfire(
    service_name='viraltracker-ui',
    environment=os.getenv('ENVIRONMENT', 'development')
)
```

**Timeline:** 1-2 hours

**Benefits:**
- Automatic tracing of all agent calls
- See which tools agents choose
- Track response times and errors
- Debugging agent behavior becomes trivial
- Visualize agent conversation flows

---

### Phase 4: Add Pydantic Evals (Optional but Recommended)

**Goal:** Regression testing for agent behavior

**New Files:**
```
viraltracker/evals/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ datasets/
‚îÇ   ‚îú‚îÄ‚îÄ twitter_dataset.py
‚îÇ   ‚îî‚îÄ‚îÄ tiktok_dataset.py
‚îú‚îÄ‚îÄ evaluators.py
‚îî‚îÄ‚îÄ run_evals.py
```

**Example: `viraltracker/evals/datasets/twitter_dataset.py`**
```python
"""Evaluation dataset for Twitter agent"""
from pydantic_evals import Dataset, Example

twitter_eval_dataset = Dataset(
    name='twitter-agent-evals',
    examples=[
        Example(
            input="Find viral tweets about Bitcoin from the last 24 hours",
            expected_output_contains=["tweets", "bitcoin", "views", "engagement"],
            expected_tool_calls=["search_twitter_tool", "get_top_tweets_tool"]
        ),
        Example(
            input="Export top tweets to CSV",
            expected_output_contains=["exported", "csv", "downloaded"],
            expected_tool_calls=["export_tweets_tool"]
        ),
        # ... more examples
    ]
)
```

**Example: `viraltracker/evals/evaluators.py`**
```python
"""Evaluators for agent responses"""
from pydantic_evals import Evaluator

def contains_keywords_evaluator(expected_keywords: list[str]) -> Evaluator:
    """Check if response contains expected keywords"""
    def evaluate(output: str) -> bool:
        return all(kw.lower() in output.lower() for kw in expected_keywords)
    return evaluate

def called_expected_tools_evaluator(expected_tools: list[str]) -> Evaluator:
    """Check if agent called expected tools"""
    def evaluate(tool_calls: list[str]) -> bool:
        return all(tool in tool_calls for tool in expected_tools)
    return evaluate
```

**Run evals:**
```bash
python -m viraltracker.evals.run_evals --dataset twitter --report ~/Downloads/eval_report.json
```

**Timeline:** 4-6 hours

**Benefits:**
- Prevent regressions when updating agent prompts
- Test tool selection accuracy
- Benchmark agent performance over time
- Generate reports on agent behavior

---

### Phase 5: Add Graph-Based Workflows (Optional)

**Goal:** Handle complex multi-step workflows with branches and joins

**When you need this:**
- Multi-step workflows with conditional logic
- Fan-out/fan-in patterns (e.g., scrape 5 platforms in parallel, then aggregate)
- Long-running workflows with state persistence

**New File:** `viraltracker/workflows/analysis_pipeline.py`

```python
"""Graph-based workflow for multi-platform analysis"""
from pydantic_graph import GraphBuilder
from viraltracker.agent.agents import (
    twitter_agent,
    tiktok_agent,
    analysis_agent
)

# Create graph
g = GraphBuilder()

@g.step
async def fetch_twitter_data(ctx):
    """Step 1: Fetch Twitter data"""
    result = await twitter_agent.run(
        "Find viral tweets about Bitcoin",
        deps=ctx.deps
    )
    return result

@g.step
async def fetch_tiktok_data(ctx):
    """Step 2: Fetch TikTok data (parallel with Twitter)"""
    result = await tiktok_agent.run(
        "Search TikTok for #bitcoin videos",
        deps=ctx.deps
    )
    return result

@g.step
async def aggregate_results(ctx, twitter_result, tiktok_result):
    """Step 3: Aggregate results from both platforms"""
    combined = {
        'twitter': twitter_result,
        'tiktok': tiktok_result
    }
    return combined

@g.step
async def generate_insights(ctx, combined_data):
    """Step 4: Generate cross-platform insights"""
    result = await analysis_agent.run(
        f"Compare Bitcoin engagement across Twitter and TikTok: {combined_data}",
        deps=ctx.deps
    )
    return result

# Define workflow
workflow = g.build(
    start_step=fetch_twitter_data,
    parallel_steps=[fetch_twitter_data, fetch_tiktok_data],
    join_step=aggregate_results,
    final_step=generate_insights
)

# Run workflow
result = await workflow.run(deps=AgentDependencies.create())
```

**Timeline:** 8-12 hours (learning curve + implementation)

**Benefits:**
- Declarative multi-step workflows
- Parallel execution built-in
- State management across steps
- Conditional branching
- Reducers for aggregating parallel results

**When to add:**
- Only if you have complex workflows that need graphs
- Current orchestrator pattern handles most cases

---

## Final Recommendations

### Immediate Actions (Do Now)

1. **‚úÖ Implement ORCHESTRATOR_REFACTOR_PLAN.md as designed** (2-3 days)
   - Your plan is excellent and follows best practices
   - No changes needed to the plan itself
   - Proceed with confidence

2. **‚ú® Add ResultCache to dependencies.py** (1-2 hours)
   - Required for orchestrator pattern
   - Already planned in ORCHESTRATOR_REFACTOR_PLAN.md

3. **‚ú® Add Logfire instrumentation** (1-2 hours)
   - Huge benefits for debugging and monitoring
   - Simple setup, automatic after
   - Will help you debug orchestrator behavior

### Short-Term Enhancements (Within 2 Weeks)

4. **‚ú® Add Model Configuration Layer** (2-3 hours)
   - Create `viraltracker/config/models.py`
   - Centralize model selection and profiles
   - Makes testing and switching models easier

5. **‚ú® Add Pydantic Evals** (4-6 hours)
   - Create `viraltracker/evals/` directory
   - Start with 5-10 eval examples per agent
   - Run before/after orchestrator refactor to compare

### Long-Term Enhancements (Future)

6. **üîÆ Add Graph-Based Workflows** (8-12 hours)
   - Only if you have complex multi-step workflows
   - Current orchestrator handles most cases
   - Can add incrementally as needs arise

7. **üîÆ Add Durable Execution** (12-20 hours)
   - Only if workflows are long-running or cross processes
   - Not needed for current use cases
   - Can add with Temporal/Prefect if needed later

---

## Comparison Table: Plans vs Best Practices

| Layer | PydanticAI Docs | Your Current State | ORCHESTRATOR Plan | Recommendation |
|-------|----------------|-------------------|------------------|----------------|
| **Level 0: Settings & Models** | ‚úÖ Centralized model config | ‚ö†Ô∏è Partial (env vars only) | ‚ö†Ô∏è Not mentioned | ‚ú® Add config/models.py |
| **Level 1: Dependencies & Outputs** | ‚úÖ deps_type + output_type | ‚úÖ AgentDependencies + models | ‚úÖ + ResultCache | ‚úÖ Perfect |
| **Level 2: Agents as Roles** | ‚úÖ One agent per role | ‚ö†Ô∏è Monolithic agent | ‚úÖ 5 specialized agents | ‚úÖ Proceed as planned |
| **Level 3: Tools & Toolsets** | ‚úÖ Organized by capability | ‚úÖ Organized by phase | ‚úÖ No change needed | ‚úÖ Good (optional: ToolSets) |
| **Level 4: Orchestration** | ‚úÖ 4 levels (single ‚Üí graphs) | ‚ö†Ô∏è Level 1 only | ‚úÖ Levels 1-3 | ‚úÖ Excellent (optional: graphs) |
| **Level 5: UI** | ‚úÖ FastAPI + Streamlit | ‚úÖ Both implemented | ‚úÖ No change | ‚úÖ Perfect |
| **Level 5: Durable Execution** | ‚úÖ Temporal/Prefect | ‚ö†Ô∏è Not implemented | ‚ö†Ô∏è Not mentioned | üîÆ Optional (future) |
| **Level 5: Evals** | ‚úÖ pydantic_evals | ‚ö†Ô∏è Not implemented | ‚ö†Ô∏è Not mentioned | ‚ú® Add for regression testing |
| **Level 5: Observability** | ‚úÖ Logfire | ‚ö†Ô∏è Basic logging only | ‚ö†Ô∏è Not mentioned | ‚ú® High priority - add Logfire |

**Legend:**
- ‚úÖ = Fully aligned with best practices
- ‚ö†Ô∏è = Partial or missing
- ‚ú® = Recommended to add
- üîÆ = Optional for future

---

## Conclusion

**Your migration plan is excellent!** üéâ

You're at **85-90% alignment** with PydanticAI best practices, which is outstanding. The ORCHESTRATOR_REFACTOR_PLAN.md is particularly well-designed and follows the multi-agent orchestration pattern perfectly.

**What to do next:**

1. **Proceed with orchestrator refactor** - Your plan is solid
2. **Add Logfire instrumentation** - Simple addition, huge benefits
3. **Add model configuration layer** - Makes testing easier
4. **Consider pydantic_evals** - Good for regression testing

**What NOT to worry about:**

- Graph-based workflows (only if you have complex workflows)
- Durable execution (only if workflows are long-running)
- ToolSets pattern (current approach is fine)

**Your FastAPI endpoints and CLI will continue working** - The orchestrator pattern is 100% backwards compatible via the `agent = orchestrator` export in `agent.py`.

**Questions?** Let me know if you'd like me to:
- Create the model configuration layer
- Add Logfire instrumentation
- Set up pydantic_evals
- Implement any other missing pieces

---

**Document Version:** 1.0
**Last Updated:** 2025-01-24
**Status:** Analysis Complete ‚úÖ
**Next Action:** Proceed with ORCHESTRATOR_REFACTOR_PLAN.md
